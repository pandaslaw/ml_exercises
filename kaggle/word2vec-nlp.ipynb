{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import exp, array, random, dot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import ceil\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from typing import List, Dict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwrod text dataset\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: (25000, 3)\n",
      "Size of unlabeled train dataset: (50000, 2)\n",
      "Size of test dataset: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/word2vec-nlp/labeledTrainData.tsv\", delimiter=\"\\t\")\n",
    "train_unlabeled = pd.read_csv(\"../data/word2vec-nlp/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"../data/word2vec-nlp/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "print(f\"Size of train dataset: {train.shape}\")\n",
    "print(f\"Size of unlabeled train dataset: {train_unlabeled.shape}\")\n",
    "print(f\"Size of test dataset: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"review\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "Data preprocessing includes the following steps:\n",
    "* Remove HTML tags\n",
    "* Remove non-letter symbols        \n",
    "* Convert to lower case, split into individual words\n",
    "* Remove stop words with the help of nltk library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(raw_review: str, remove_stopwords: bool = False) -> str:\n",
    "    \"\"\"Converts a raw review to a processed string of words.\"\"\"\n",
    "\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    words = letters_only.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', 'moonwalker', 'is', 'part', 'biography', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'mj', 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', 'kay', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'mj', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', 'some', 'may', 'call', 'mj', 'an', 'egotist', 'for', 'consenting', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'mj', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him', 'the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pesci', 'is', 'convincing', 'as', 'a', 'psychopathic', 'all', 'powerful', 'drug', 'lord', 'why', 'he', 'wants', 'mj', 'dead', 'so', 'bad', 'is', 'beyond', 'me', 'because', 'mj', 'overheard', 'his', 'plans', 'nah', 'joe', 'pesci', 's', 'character', 'ranted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunno', 'maybe', 'he', 'just', 'hates', 'mj', 's', 'music', 'lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'mj', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence', 'also', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kiddy', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene', 'bottom', 'line', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'mj', 'on', 'one', 'level', 'or', 'another', 'which', 'i', 'think', 'is', 'most', 'people', 'if', 'not', 'then', 'stay', 'away', 'it', 'does', 'try', 'and', 'give', 'off', 'a', 'wholesome', 'message', 'and', 'ironically', 'mj', 's', 'bestest', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl', 'michael', 'jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty', 'well', 'with', 'all', 'the', 'attention', 'i', 've', 'gave', 'this', 'subject', 'hmmm', 'well', 'i', 'don', 't', 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors', 'i', 'know', 'this', 'for', 'a', 'fact', 'he', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sickest', 'liars', 'i', 'hope', 'he', 'is', 'not', 'the', 'latter']\n"
     ]
    }
   ],
   "source": [
    "clean_review = review_to_wordlist(train[\"review\"][0])\n",
    "print(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\venvs\\venv310_basic\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 25000 of 25000\n",
      "All reviews have been cleaned out.\n"
     ]
    }
   ],
   "source": [
    "num_reviews = train[\"review\"].size\n",
    "clean_train_reviews = []\n",
    "\n",
    "for i in range(0, num_reviews):\n",
    "    if((i + 1) % 5000 == 0):\n",
    "        print(f\"Review {i+1} of {num_reviews}\")\n",
    "    clean_train_reviews.append(review_to_wordlist(train[\"review\"][i]))\n",
    "print(\"All reviews have been cleaned out.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences(review: str, tokenizer: PunktSentenceTokenizer, \n",
    "                        remove_stopwords: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a review into parsed sentences. Returns a list of sentences, \n",
    "    where each sentence is a list of words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\venvs\\venv310_basic\\lib\\site-packages\\bs4\\__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in train_unlabeled[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 20:01:36,808 : INFO : collecting all words and their counts\n",
      "2022-10-01 20:01:36,809 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-10-01 20:01:36,832 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2022-10-01 20:01:36,858 : INFO : PROGRESS: at sentence #20000, processed 451842 words, keeping 24946 word types\n",
      "2022-10-01 20:01:36,882 : INFO : PROGRESS: at sentence #30000, processed 671054 words, keeping 30029 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 20:01:36,907 : INFO : PROGRESS: at sentence #40000, processed 897625 words, keeping 34348 word types\n",
      "2022-10-01 20:01:36,932 : INFO : PROGRESS: at sentence #50000, processed 1120159 words, keeping 37805 word types\n",
      "2022-10-01 20:01:36,957 : INFO : PROGRESS: at sentence #60000, processed 1340914 words, keeping 40769 word types\n",
      "2022-10-01 20:01:36,978 : INFO : PROGRESS: at sentence #70000, processed 1564764 words, keeping 43362 word types\n",
      "2022-10-01 20:01:37,010 : INFO : PROGRESS: at sentence #80000, processed 1784083 words, keeping 45745 word types\n",
      "2022-10-01 20:01:37,036 : INFO : PROGRESS: at sentence #90000, processed 2007591 words, keeping 48167 word types\n",
      "2022-10-01 20:01:37,059 : INFO : PROGRESS: at sentence #100000, processed 2228932 words, keeping 50220 word types\n",
      "2022-10-01 20:01:37,086 : INFO : PROGRESS: at sentence #110000, processed 2449108 words, keeping 52105 word types\n",
      "2022-10-01 20:01:37,114 : INFO : PROGRESS: at sentence #120000, processed 2671053 words, keeping 54142 word types\n",
      "2022-10-01 20:01:37,138 : INFO : PROGRESS: at sentence #130000, processed 2896619 words, keeping 55864 word types\n",
      "2022-10-01 20:01:37,168 : INFO : PROGRESS: at sentence #140000, processed 3113151 words, keeping 57400 word types\n",
      "2022-10-01 20:01:37,194 : INFO : PROGRESS: at sentence #150000, processed 3338976 words, keeping 59085 word types\n",
      "2022-10-01 20:01:37,216 : INFO : PROGRESS: at sentence #160000, processed 3561883 words, keeping 60646 word types\n",
      "2022-10-01 20:01:37,242 : INFO : PROGRESS: at sentence #170000, processed 3785579 words, keeping 62122 word types\n",
      "2022-10-01 20:01:37,264 : INFO : PROGRESS: at sentence #180000, processed 4007541 words, keeping 63534 word types\n",
      "2022-10-01 20:01:37,291 : INFO : PROGRESS: at sentence #190000, processed 4232985 words, keeping 64848 word types\n",
      "2022-10-01 20:01:37,312 : INFO : PROGRESS: at sentence #200000, processed 4456605 words, keeping 66130 word types\n",
      "2022-10-01 20:01:37,338 : INFO : PROGRESS: at sentence #210000, processed 4678976 words, keeping 67439 word types\n",
      "2022-10-01 20:01:37,361 : INFO : PROGRESS: at sentence #220000, processed 4903687 words, keeping 68743 word types\n",
      "2022-10-01 20:01:37,387 : INFO : PROGRESS: at sentence #230000, processed 5125705 words, keeping 70006 word types\n",
      "2022-10-01 20:01:37,409 : INFO : PROGRESS: at sentence #240000, processed 5352560 words, keeping 71213 word types\n",
      "2022-10-01 20:01:37,433 : INFO : PROGRESS: at sentence #250000, processed 5566898 words, keeping 72387 word types\n",
      "2022-10-01 20:01:37,464 : INFO : PROGRESS: at sentence #260000, processed 5787063 words, keeping 73525 word types\n",
      "2022-10-01 20:01:37,490 : INFO : PROGRESS: at sentence #270000, processed 6010187 words, keeping 74846 word types\n",
      "2022-10-01 20:01:37,513 : INFO : PROGRESS: at sentence #280000, processed 6236056 words, keeping 76473 word types\n",
      "2022-10-01 20:01:37,540 : INFO : PROGRESS: at sentence #290000, processed 6459398 words, keeping 77894 word types\n",
      "2022-10-01 20:01:37,563 : INFO : PROGRESS: at sentence #300000, processed 6685697 words, keeping 79236 word types\n",
      "2022-10-01 20:01:37,591 : INFO : PROGRESS: at sentence #310000, processed 6910358 words, keeping 80553 word types\n",
      "2022-10-01 20:01:37,614 : INFO : PROGRESS: at sentence #320000, processed 7135812 words, keeping 81858 word types\n",
      "2022-10-01 20:01:37,640 : INFO : PROGRESS: at sentence #330000, processed 7359631 words, keeping 83103 word types\n",
      "2022-10-01 20:01:37,663 : INFO : PROGRESS: at sentence #340000, processed 7587975 words, keeping 84327 word types\n",
      "2022-10-01 20:01:37,689 : INFO : PROGRESS: at sentence #350000, processed 7811665 words, keeping 85504 word types\n",
      "2022-10-01 20:01:37,716 : INFO : PROGRESS: at sentence #360000, processed 8032212 words, keeping 86674 word types\n",
      "2022-10-01 20:01:37,743 : INFO : PROGRESS: at sentence #370000, processed 8261814 words, keeping 87781 word types\n",
      "2022-10-01 20:01:37,774 : INFO : PROGRESS: at sentence #380000, processed 8486429 words, keeping 88934 word types\n",
      "2022-10-01 20:01:37,799 : INFO : PROGRESS: at sentence #390000, processed 8716600 words, keeping 89949 word types\n",
      "2022-10-01 20:01:37,825 : INFO : PROGRESS: at sentence #400000, processed 8938748 words, keeping 90997 word types\n",
      "2022-10-01 20:01:37,851 : INFO : PROGRESS: at sentence #410000, processed 9160457 words, keeping 91948 word types\n",
      "2022-10-01 20:01:37,879 : INFO : PROGRESS: at sentence #420000, processed 9382745 words, keeping 92995 word types\n",
      "2022-10-01 20:01:37,905 : INFO : PROGRESS: at sentence #430000, processed 9611256 words, keeping 93993 word types\n",
      "2022-10-01 20:01:37,928 : INFO : PROGRESS: at sentence #440000, processed 9836405 words, keeping 94990 word types\n",
      "2022-10-01 20:01:37,953 : INFO : PROGRESS: at sentence #450000, processed 10062007 words, keeping 96113 word types\n",
      "2022-10-01 20:01:37,988 : INFO : PROGRESS: at sentence #460000, processed 10294694 words, keeping 97143 word types\n",
      "2022-10-01 20:01:38,017 : INFO : PROGRESS: at sentence #470000, processed 10522661 words, keeping 98018 word types\n",
      "2022-10-01 20:01:38,043 : INFO : PROGRESS: at sentence #480000, processed 10744660 words, keeping 98945 word types\n",
      "2022-10-01 20:01:38,068 : INFO : PROGRESS: at sentence #490000, processed 10970287 words, keeping 99936 word types\n",
      "2022-10-01 20:01:38,095 : INFO : PROGRESS: at sentence #500000, processed 11192255 words, keeping 100858 word types\n",
      "2022-10-01 20:01:38,119 : INFO : PROGRESS: at sentence #510000, processed 11417273 words, keeping 101752 word types\n",
      "2022-10-01 20:01:38,148 : INFO : PROGRESS: at sentence #520000, processed 11641610 words, keeping 102667 word types\n",
      "2022-10-01 20:01:38,171 : INFO : PROGRESS: at sentence #530000, processed 11866279 words, keeping 103477 word types\n",
      "2022-10-01 20:01:38,200 : INFO : PROGRESS: at sentence #540000, processed 12090485 words, keeping 104343 word types\n",
      "2022-10-01 20:01:38,223 : INFO : PROGRESS: at sentence #550000, processed 12316842 words, keeping 105213 word types\n",
      "2022-10-01 20:01:38,249 : INFO : PROGRESS: at sentence #560000, processed 12539560 words, keeping 106038 word types\n",
      "2022-10-01 20:01:38,273 : INFO : PROGRESS: at sentence #570000, processed 12767139 words, keeping 106857 word types\n",
      "2022-10-01 20:01:38,299 : INFO : PROGRESS: at sentence #580000, processed 12990563 words, keeping 107726 word types\n",
      "2022-10-01 20:01:38,326 : INFO : PROGRESS: at sentence #590000, processed 13215773 words, keeping 108567 word types\n",
      "2022-10-01 20:01:38,348 : INFO : PROGRESS: at sentence #600000, processed 13436983 words, keeping 109273 word types\n",
      "2022-10-01 20:01:38,375 : INFO : PROGRESS: at sentence #610000, processed 13660140 words, keeping 110153 word types\n",
      "2022-10-01 20:01:38,397 : INFO : PROGRESS: at sentence #620000, processed 13885543 words, keeping 110907 word types\n",
      "2022-10-01 20:01:38,426 : INFO : PROGRESS: at sentence #630000, processed 14108834 words, keeping 111679 word types\n",
      "2022-10-01 20:01:38,447 : INFO : PROGRESS: at sentence #640000, processed 14331863 words, keeping 112486 word types\n",
      "2022-10-01 20:01:38,475 : INFO : PROGRESS: at sentence #650000, processed 14557568 words, keeping 113265 word types\n",
      "2022-10-01 20:01:38,497 : INFO : PROGRESS: at sentence #660000, processed 14780199 words, keeping 114019 word types\n",
      "2022-10-01 20:01:38,526 : INFO : PROGRESS: at sentence #670000, processed 15004101 words, keeping 114710 word types\n",
      "2022-10-01 20:01:38,553 : INFO : PROGRESS: at sentence #680000, processed 15229279 words, keeping 115426 word types\n",
      "2022-10-01 20:01:38,582 : INFO : PROGRESS: at sentence #690000, processed 15453670 words, keeping 116227 word types\n",
      "2022-10-01 20:01:38,607 : INFO : PROGRESS: at sentence #700000, processed 15681284 words, keeping 117012 word types\n",
      "2022-10-01 20:01:38,638 : INFO : PROGRESS: at sentence #710000, processed 15904248 words, keeping 117679 word types\n",
      "2022-10-01 20:01:38,660 : INFO : PROGRESS: at sentence #720000, processed 16130167 words, keeping 118335 word types\n",
      "2022-10-01 20:01:38,691 : INFO : PROGRESS: at sentence #730000, processed 16356902 words, keeping 119048 word types\n",
      "2022-10-01 20:01:38,713 : INFO : PROGRESS: at sentence #740000, processed 16578050 words, keeping 119739 word types\n",
      "2022-10-01 20:01:38,740 : INFO : PROGRESS: at sentence #750000, processed 16798326 words, keeping 120397 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 20:01:38,762 : INFO : PROGRESS: at sentence #760000, processed 17015183 words, keeping 121044 word types\n",
      "2022-10-01 20:01:38,791 : INFO : PROGRESS: at sentence #770000, processed 17244246 words, keeping 121775 word types\n",
      "2022-10-01 20:01:38,821 : INFO : PROGRESS: at sentence #780000, processed 17473909 words, keeping 122466 word types\n",
      "2022-10-01 20:01:38,852 : INFO : PROGRESS: at sentence #790000, processed 17701995 words, keeping 123164 word types\n",
      "2022-10-01 20:01:38,865 : INFO : collected 123504 word types from a corpus of 17798269 raw words and 794335 sentences\n",
      "2022-10-01 20:01:38,866 : INFO : Creating a fresh vocabulary\n",
      "2022-10-01 20:01:38,917 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16490 unique words (13.35% of original 123504, drops 107014)', 'datetime': '2022-10-01T20:01:38.917141', 'gensim': '4.2.0', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-01 20:01:38,918 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 17239124 word corpus (96.86% of original 17798269, drops 559145)', 'datetime': '2022-10-01T20:01:38.918142', 'gensim': '4.2.0', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-01 20:01:38,957 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2022-10-01 20:01:38,960 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2022-10-01 20:01:38,961 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12749797.32004592 word corpus (74.0%% of prior 17239124)', 'datetime': '2022-10-01T20:01:38.961217', 'gensim': '4.2.0', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-01 20:01:39,021 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2022-10-01 20:01:39,022 : INFO : resetting layer weights\n",
      "2022-10-01 20:01:39,038 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-10-01T20:01:39.037426', 'gensim': '4.2.0', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "2022-10-01 20:01:39,040 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-10-01T20:01:39.040505', 'gensim': '4.2.0', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-10-01 20:01:40,051 : INFO : EPOCH 0 - PROGRESS: at 12.35% examples, 1558766 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:01:41,056 : INFO : EPOCH 0 - PROGRESS: at 26.18% examples, 1652275 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:01:42,058 : INFO : EPOCH 0 - PROGRESS: at 40.12% examples, 1690942 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:01:43,061 : INFO : EPOCH 0 - PROGRESS: at 54.53% examples, 1727454 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:01:44,064 : INFO : EPOCH 0 - PROGRESS: at 68.87% examples, 1748529 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:01:45,071 : INFO : EPOCH 0 - PROGRESS: at 83.45% examples, 1765254 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:01:46,071 : INFO : EPOCH 0 - PROGRESS: at 97.73% examples, 1773368 words/s, in_qsize 7, out_qsize 1\n",
      "2022-10-01 20:01:46,235 : INFO : EPOCH 0: training on 17798269 raw words (12750270 effective words) took 7.2s, 1773735 effective words/s\n",
      "2022-10-01 20:01:47,245 : INFO : EPOCH 1 - PROGRESS: at 13.09% examples, 1650147 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:01:48,252 : INFO : EPOCH 1 - PROGRESS: at 26.13% examples, 1645007 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:01:49,255 : INFO : EPOCH 1 - PROGRESS: at 37.62% examples, 1581464 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:01:50,256 : INFO : EPOCH 1 - PROGRESS: at 48.26% examples, 1526787 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:01:51,260 : INFO : EPOCH 1 - PROGRESS: at 61.29% examples, 1554709 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:01:52,264 : INFO : EPOCH 1 - PROGRESS: at 74.34% examples, 1572468 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:01:53,267 : INFO : EPOCH 1 - PROGRESS: at 87.80% examples, 1592433 words/s, in_qsize 6, out_qsize 1\n",
      "2022-10-01 20:01:54,185 : INFO : EPOCH 1: training on 17798269 raw words (12748844 effective words) took 7.9s, 1604469 effective words/s\n",
      "2022-10-01 20:01:55,194 : INFO : EPOCH 2 - PROGRESS: at 13.60% examples, 1715871 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:01:56,201 : INFO : EPOCH 2 - PROGRESS: at 27.57% examples, 1738554 words/s, in_qsize 7, out_qsize 1\n",
      "2022-10-01 20:01:57,201 : INFO : EPOCH 2 - PROGRESS: at 41.53% examples, 1749959 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:01:58,202 : INFO : EPOCH 2 - PROGRESS: at 54.81% examples, 1737712 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:01:59,203 : INFO : EPOCH 2 - PROGRESS: at 67.91% examples, 1725810 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:02:00,207 : INFO : EPOCH 2 - PROGRESS: at 81.53% examples, 1726858 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:02:01,211 : INFO : EPOCH 2 - PROGRESS: at 94.89% examples, 1722434 words/s, in_qsize 6, out_qsize 1\n",
      "2022-10-01 20:02:01,583 : INFO : EPOCH 2: training on 17798269 raw words (12752046 effective words) took 7.4s, 1724655 effective words/s\n",
      "2022-10-01 20:02:02,594 : INFO : EPOCH 3 - PROGRESS: at 11.73% examples, 1478579 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:02:03,594 : INFO : EPOCH 3 - PROGRESS: at 24.77% examples, 1565216 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:02:04,601 : INFO : EPOCH 3 - PROGRESS: at 38.23% examples, 1608968 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:02:05,602 : INFO : EPOCH 3 - PROGRESS: at 51.64% examples, 1635203 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:02:06,603 : INFO : EPOCH 3 - PROGRESS: at 65.11% examples, 1653694 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:02:07,612 : INFO : EPOCH 3 - PROGRESS: at 78.79% examples, 1666451 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:02:08,614 : INFO : EPOCH 3 - PROGRESS: at 92.56% examples, 1679314 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:02:09,155 : INFO : EPOCH 3: training on 17798269 raw words (12749424 effective words) took 7.6s, 1684649 effective words/s\n",
      "2022-10-01 20:02:10,160 : INFO : EPOCH 4 - PROGRESS: at 14.56% examples, 1845314 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:02:11,165 : INFO : EPOCH 4 - PROGRESS: at 28.93% examples, 1829375 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:02:12,166 : INFO : EPOCH 4 - PROGRESS: at 43.24% examples, 1826397 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:02:13,169 : INFO : EPOCH 4 - PROGRESS: at 57.78% examples, 1835091 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:02:14,169 : INFO : EPOCH 4 - PROGRESS: at 72.43% examples, 1842843 words/s, in_qsize 7, out_qsize 0\n",
      "2022-10-01 20:02:15,170 : INFO : EPOCH 4 - PROGRESS: at 87.02% examples, 1845405 words/s, in_qsize 8, out_qsize 0\n",
      "2022-10-01 20:02:16,059 : INFO : EPOCH 4: training on 17798269 raw words (12751478 effective words) took 6.9s, 1848052 effective words/s\n",
      "2022-10-01 20:02:16,060 : INFO : Word2Vec lifecycle event {'msg': 'training on 88991345 raw words (63752062 effective words) took 37.0s, 1722133 effective words/s', 'datetime': '2022-10-01T20:02:16.060527', 'gensim': '4.2.0', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-10-01 20:02:16,061 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16490, vector_size=300, alpha=0.025>', 'datetime': '2022-10-01T20:02:16.061527', 'gensim': '4.2.0', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_3924\\1671025947.py:21: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n",
      "2022-10-01 20:02:16,068 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 20:02:16,071 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-10-01T20:02:16.071527', 'gensim': '4.2.0', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'saving'}\n",
      "2022-10-01 20:02:16,071 : INFO : not storing attribute cum_table\n",
      "2022-10-01 20:02:16,100 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences, workers=num_workers, vector_size=num_features, \n",
    "    min_count=min_word_count, window=context, sample=downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# save the model, it can loaded later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.6104881763458252), ('lad', 0.5949812531471252), ('lady', 0.5708168745040894), ('millionaire', 0.5419608354568481), ('farmer', 0.5168347954750061), ('monk', 0.5143375396728516), ('guy', 0.5069648623466492), ('person', 0.5006126761436462), ('men', 0.4968082010746002), ('soldier', 0.49505871534347534)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"man\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(words: List[str], model: word2vec.Word2Vec, num_features: int) -> np.array:\n",
    "    \"\"\"Averages all of the word vectors in a given paragraph.\"\"\"\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "\n",
    "    # index_to_key is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    feature_vector = np.divide(feature_vector, nwords)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def get_avg_feature_vectors(reviews: List[List[str]], model: word2vec.Word2Vec, num_features: int) -> np.array:\n",
    "    \"\"\"Calculates the average feature vector for each review and returns a 2D numpy array.\"\"\"\n",
    "\n",
    "    counter = 0\n",
    "    review_feature_vectors = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "        if counter % 5000 == 0:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "        review_feature_vectors[counter] = get_feature_vector(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "    \n",
    "    return review_feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\venvs\\venv310_basic\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# Calculate average feature vectors for training and testing sets, use stop word removal\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "trainDataVecs = get_avg_feature_vectors(clean_train_reviews, model, num_features)\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist( review, remove_stopwords=True))\n",
    "\n",
    "test_data_vectors = get_avg_feature_vectors(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Random Forest classifier with 100 trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(trainDataVecs, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          1\n",
       "1    \"8348_2\"          0\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          0\n",
       "4   \"12128_7\"          1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = forest.predict(test_data_vectors)\n",
    "\n",
    "submission = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "submission.to_csv(\"../data/word2vec-nlp/Word2Vec_AverageVectors.csv\", index=False, quoting=3)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model (Clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Means clustering took 938.73 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.wv.vectors\n",
    "num_clusters = word_vectors.shape[0] // 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "print(f\"K Means clustering took {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each vocabulary word to a cluster number\n",
    "word_centroid_map = dict(zip(model.wv.index_to_key, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['fulfilled', 'purity']\n",
      "\n",
      "Cluster 1\n",
      "['fields', 'shanghai', 'buffalo', 'stagecoach', 'ka', 'monument']\n",
      "\n",
      "Cluster 2\n",
      "['fatal', 'drowning', 'unconscious', 'suicidal', 'casually', 'paralyzed', 'comatose', 'poisoning', 'katrina', 'gutted']\n",
      "\n",
      "Cluster 3\n",
      "['bateman']\n",
      "\n",
      "Cluster 4\n",
      "['vacuous', 'mismatched', 'colorless']\n",
      "\n",
      "Cluster 5\n",
      "['background', 'bands', 'instruments']\n",
      "\n",
      "Cluster 6\n",
      "['nielsen', 'caron', 'comedienne']\n",
      "\n",
      "Cluster 7\n",
      "['julian', 'noah', 'domino', 'shane', 'milo', 'einstein', 'meadows', 'reyes']\n",
      "\n",
      "Cluster 8\n",
      "['drive', 'driving', 'drives', 'passes', 'drove']\n",
      "\n",
      "Cluster 9\n",
      "['caught', 'picked', 'signed']\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in range(0,10):\n",
    "    print(f\"\\nCluster {cluster}\")\n",
    "    words = []\n",
    "    for i in range(0, len(word_centroid_map.values())):\n",
    "        if list(word_centroid_map.values())[i] == cluster:\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist: List[str], word_centroid_map: Dict[str, int]) -> np.array:\n",
    "    \"\"\"\n",
    "    Loop over the words in the review. If the word is in the vocabulary,\n",
    "    find which cluster it belongs to, and increment that cluster count by one.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the prediction (Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          1\n",
       "1    \"8348_2\"          0\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          0\n",
       "4   \"12128_7\"          1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = forest.predict(test_centroids)\n",
    "\n",
    "submission_clustering = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": result})\n",
    "submission_clustering.to_csv(\"../data/word2vec-nlp/BagOfCentroids.csv\", index=False, quoting=3)\n",
    "submission_clustering.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
